{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from typing_extensions import Literal, TypedDict\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "import os\n",
    "\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x: list[str] = []\n",
    "x.append(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack 30\n",
      "Rubbish!\n",
      "Rubbish2!\n",
      "I am the child!\n"
     ]
    }
   ],
   "source": [
    "class ParentModel:\n",
    "    name = \"Jack\"\n",
    "    def __init__(self, age):\n",
    "        self.age = age\n",
    "\n",
    "    def print_rubbish(self):\n",
    "        print(\"Rubbish!\")\n",
    "        return \"Rubbish2!\"\n",
    "\n",
    "class ChildModel(ParentModel):\n",
    "    # def __init__(self, age):\n",
    "    #     super().__init__(age)\n",
    "    #     super().print_rubbish()\n",
    "\n",
    "    # def print_rubbish2(self):\n",
    "    #     super().print_rubbish()\n",
    "    def __repr__(self):\n",
    "        return \"I am the child!\"\n",
    "    pass\n",
    "\n",
    "child = ChildModel(age=30)\n",
    "print(child.name, child.age)\n",
    "print(child.print_rubbish())\n",
    "print(child)\n",
    "# Not using super still allows ChildModel to inherit ParentModel's methods and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "C\n",
      "B\n",
      "D\n"
     ]
    }
   ],
   "source": [
    "class A:\n",
    "    def __init__(self):\n",
    "        print(\"A\")\n",
    "        \n",
    "class B(A):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"B\")\n",
    "\n",
    "class C(A):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"C\")\n",
    "\n",
    "class D(B, C):  # Multiple inheritance\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"D\")\n",
    "\n",
    "test =D()\n",
    "# Output:\n",
    "# A\n",
    "# C\n",
    "# B\n",
    "# D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParentModel(BaseModel):\n",
    "    name: str\n",
    "\n",
    "class ChildModel(ParentModel):\n",
    "    age: int  # Inherits name but doesn't need to call super()\n",
    "\n",
    "child = ChildModel(name=\"Alice\", age=30)\n",
    "print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': <class 'str'>, 'age': <class 'int'>} {'name': 'Alice', 'age': 30}\n",
      "ChildModel(name='Alice', age=30)\n"
     ]
    }
   ],
   "source": [
    "from typing import get_type_hints\n",
    "\n",
    "class BaseModel:\n",
    "    def __init__(self, **kwargs):\n",
    "        type_hints = get_type_hints(self.__class__)  # Get class attributes & expected types\n",
    "        print(type_hints, kwargs)\n",
    "        for field, field_type in type_hints.items():\n",
    "            if field not in kwargs:\n",
    "                raise ValueError(f\"Missing required field: {field}\")\n",
    "            if not isinstance(kwargs[field], field_type):\n",
    "                raise TypeError(f\"Expected {field} to be {field_type}, got {type(kwargs[field])}\")\n",
    "            setattr(self, field, kwargs[field])  # Dynamically set attributes\n",
    "\n",
    "    def __repr__(self):\n",
    "        fields = \", \".join(f\"{k}={v!r}\" for k, v in self.__dict__.items())\n",
    "        return f\"{self.__class__.__name__}({fields})\"\n",
    "\n",
    "# Example Usage\n",
    "class ChildModel(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "# Creating an instance\n",
    "child = ChildModel(name=\"Alice\", age=30)\n",
    "print(child)  # Output: ChildModel(name='Alice', age=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=OPENAI_API_KEY)\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\", anthropic_api_key=ANTHROPIC_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputState(TypedDict):\n",
    "    user_input: str\n",
    "    def __init__(self):\n",
    "        self.check = \"checking if this works\"\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    graph_output: str\n",
    "    foo: str\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    foo: str\n",
    "    user_input: str\n",
    "    graph_output: str\n",
    "\n",
    "class PrivateState(TypedDict):\n",
    "    bar: str\n",
    "\n",
    "def node_1(state: InputState) -> OverallState:\n",
    "    # Write to OverallState\n",
    "    print(state)\n",
    "    return {\"foo\": state[\"user_input\"] + \" name\"}\n",
    "\n",
    "def node_2(state: OverallState) -> PrivateState:\n",
    "    # Read from OverallState, write to PrivateState\n",
    "    return {\"bar\": state[\"foo\"] + \" is\"}\n",
    "\n",
    "def node_3(state: PrivateState) -> OutputState:\n",
    "    # Read from PrivateState, write to OutputState\n",
    "    return {\"graph_output\": state[\"bar\"] + \" Lance\",\n",
    "            \"foo\": \"OVERWRITE\"}\n",
    "\n",
    "builder = StateGraph(OverallState,input=InputState,output=OutputState)\n",
    "builder.add_node(\"node_1\", node_1)\n",
    "builder.add_node(\"node_2\", node_2)\n",
    "builder.add_node(\"node_3\", node_3)\n",
    "\n",
    "builder.add_edge(START, \"node_1\")\n",
    "builder.add_edge(\"node_1\", \"node_2\")\n",
    "builder.add_edge(\"node_2\", \"node_3\")\n",
    "builder.add_edge(\"node_3\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "# Show the agent\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "graph.invoke({\"user_input\":\"My\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for structured output\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Query that is optimized web search.\")\n",
    "    justification: str = Field(\n",
    "        None, description=\"Why this query is relevant to the user's request.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "structured_llm = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "# Invoke the augmented LLM\n",
    "output = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\n",
    "\n",
    "# Define a tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    return a * b\n",
    "\n",
    "# Augment the LLM with tools\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Invoke the LLM with input that triggers the tool call\n",
    "msg = llm_with_tools.invoke(\"What is 2 times 3?\")\n",
    "\n",
    "# Get the tool call\n",
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "# Augment the LLM with tools\n",
    "tools = [add, multiply, divide]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def llm_call(state: MessagesState):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "    print(\"STATE MESSAGE\", state['messages'])\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm_with_tools.invoke(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "                    )\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def tool_node(state: dict):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=\"observation\", tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}\n",
    "\n",
    "\n",
    "# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n",
    "def should_continue(state: MessagesState) -> Literal[\"environment\", END]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"Action\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"environment\", tool_node)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        # Name returned by should_continue : Name of next node to visit\n",
    "        \"Action\": \"environment\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"environment\", \"llm_call\")\n",
    "\n",
    "# Compile the agent\n",
    "agent = agent_builder.compile()\n",
    "\n",
    "# Show the agent\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n",
    "# Invoke\n",
    "messages = [HumanMessage(content=\"Add 3 and 4. Then multiply the result by 10\")]\n",
    "messages = agent.invoke({\"messages\": messages})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
